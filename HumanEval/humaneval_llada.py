import torch
import json
import os
from transformers import AutoModel, AutoTokenizer
from human_eval.data import read_problems, write_jsonl
from human_eval.evaluation import evaluate_functional_correctness
import time
from contextlib import contextmanager
from llada_generate import generate

# Define hyperparameters
steps = 512
gen_length = 512
block_length = 64
temperature = 0.4

@contextmanager
def multi_timer():
    total_time = 0.0
    start_time = None
    
    class Timer:
        def start(self):
            nonlocal start_time
            if start_time is not None:
                raise RuntimeError("Timer has already started. Please stop the current timer first.")
            start_time = time.perf_counter()
        
        def stop(self):
            nonlocal total_time, start_time
            if start_time is None:
                raise RuntimeError("Please start the timer first.")
            end_time = time.perf_counter()
            total_time += end_time - start_time
            start_time = None
            
        def get_total(self):
            """Get the currently accumulated total time"""
            return total_time
    
    try:
        yield Timer()
    finally:
        print(f"Total execution time of all timed statements: {total_time:.6f} seconds")

# Load model and tokenizer
def append_number_to_file(filename, number):
    """
    Append a number to the end of a file
    
    Parameters:
        filename (str): Target file path
        number: Number to be appended (supports integer, float, and other numeric types)
    """
    try:
        # Open the file in append mode; create the file if it does not exist
        with open(filename, 'a') as file:
            # Append the number, and add a newline character if needed
            file.write(f"{number}\n")  # Add a newline to make each number occupy a separate line
        print(f"Successfully appended number {number} to file {filename}")
    except Exception as e:
        print(f"Error occurred while appending number: {e}")

model_path = "/xxx/xxx/LLaDA-8B-Instruct"
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to("cuda:6").eval()

# Define list of EOS markers
EOS_MARKERS = [
    "<|endoftext|>",
    "<|eot_id|>",
    "</s>",
    "\nif __name__",
    "\ndef main(",
    "\nprint(",
    "\n```",
    "\nassert ",
    "!!!"
]

# Load HumanEval dataset
problems = read_problems()

# List to store generated results
results = []

# Process each problem
with multi_timer() as timer:
    for problem_id, problem in problems.items():
        print(f"Processing problem: {problem_id}")
        
        # Extract problem prompt
        prompt = problem['prompt']
        
        # Build new prompt template
        prompt_template = f"""
<|startoftext|><|start_header_id|>user<|end_header_id|>

You are an intelligent programming assistant to produce Python algorithmic solutions. Can you complete the following Python function?
```python
{prompt}
```
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```python
{prompt}
"""
        # Directly tokenize the template
        inputs = tokenizer(
            prompt_template,
            return_tensors="pt",
            truncation=True,
            max_length=2048  # Adjust according to the model's maximum length
        )
        input_ids = inputs.input_ids.to(device="cuda:6")
        attention_mask = inputs.attention_mask.to(device="cuda:6")
        
        # Generate code (using defined hyperparameters)
        timer.start()
        outputs = generate(
            model, 
            input_ids, 
            steps=steps, 
            gen_length=gen_length, 
            block_length=block_length, 
            temperature=temperature, 
            cfg_scale=0., 
            remasking='random'
        )
        timer.stop()
        
        # Decode generated content (exclude the input part)
        generated_text = tokenizer.batch_decode(
            outputs[:, input_ids.shape[1]:],  # Extract the part newly generated by the model (exclude input)
            skip_special_tokens=True)[0]
        
        # Find the earliest occurrence of an EOS marker and truncate the text
        min_index = None
        for marker in EOS_MARKERS:
            index = generated_text.find(marker)
            if index != -1:
                if min_index is None or index < min_index:
                    min_index = index
        
        # Truncate the result
        if min_index is not None:
            extracted_code = generated_text[:min_index]
        else:
            extracted_code = generated_text  # If no EOS marker is found, use the entire content
        
        print(extracted_code)
        
        # Save the result
        results.append({
            "task_id": problem_id,
            "completion": extracted_code
        })
    p = timer.get_total()

# Define result file path (includes hyperparameter information)
output_file = f"humaneval_llada_RQ1_steps{steps}_gen{gen_length}_block{block_length}_temp{temperature}.jsonl"
append_number_to_file("speed.txt", f"{str(p)} {output_file}")

# Save generated results to JSONL file
with open(output_file, mode='w', encoding='utf-8') as f:
    for result in results:
        json.dump(result, f)
        f.write('\n')

# Evaluate generated code
print("Starting evaluation...")
evaluation_results = evaluate_functional_correctness(output_file, k=[1, 10, 100], n_workers=4)

# Print evaluation results
print("\nEvaluation Results:")
for k, v in evaluation_results.items():
    print(f"k={k}: {v}")

print(f"Results saved to {output_file}")