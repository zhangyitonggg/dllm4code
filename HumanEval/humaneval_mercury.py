import torch
import json
import os
import re
import requests
from human_eval.data import read_problems, write_jsonl
from human_eval.evaluation import evaluate_functional_correctness
import time
from contextlib import contextmanager

# Configure Mercury API
MERCURY_API_URL = "https://api.inceptionlabs.ai/v1/chat/completions"
API_KEY = "sk_xxx"  # Please replace with your API key

# List of max_new_tokens values to test
MAX_NEW_TOKENS_LIST = [1024]

@contextmanager
def multi_timer():
    total_time = 0.0
    start_time = None
    
    class Timer:
        def start(self):
            nonlocal start_time
            if start_time is not None:
                raise RuntimeError("Timer has already started. Please stop the current timer first.")
            start_time = time.perf_counter()
        
        def stop(self):
            nonlocal total_time, start_time
            if start_time is None:
                raise RuntimeError("Please start the timer first.")
            end_time = time.perf_counter()
            total_time += end_time - start_time
            start_time = None
            
        def get_total(self):
            """Get the currently accumulated total time"""
            return total_time
    
    try:
        yield Timer()
    finally:
        print(f"Total execution time of all timed statements: {total_time:.6f} seconds")

def append_number_to_file(filename, number):
    """
    Append a number to the end of a file
    
    Parameters:
        filename (str): Target file path
        number: Number to be appended (supports integer, float, and other numeric types)
    """
    try:
        with open(filename, 'a') as file:
            file.write(f"{number}\n")
        print(f"Successfully appended number {number} to file {filename}")
    except Exception as e:
        print(f"Error occurred while appending number: {e}")

def call_mercury_api(prompt, max_new_tokens):
    """Call Mercury API to generate code"""
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    
    data = {
        "model": "mercury-coder-small",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_new_tokens,
        "temperature": 0.0,
        "presence_penalty": 0.0,
        "frequency_penalty": 0.0,
        "top_p": 0.95
    }
    
    try:
        response = requests.post(MERCURY_API_URL, headers=headers, json=data)
        response.raise_for_status()  # Raise HTTP error if occurred
        # print(response.json()["choices"][0]["message"]["content"])
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        print(f"API call error: {e}")
        return ""

# Define list of EOS markers
EOS_MARKERS = [
    "<|endoftext|>",
    "<|eot_id|>",
    "</s>",
    "\nif __name__",
    "\ndef main(",
    "\nprint(",
    "\n#",
    "\n```",
    "\nassert "
]

def extract_completion(generated_code, prompt):
    """
    Extract the part of generated code that does not belong to the prompt prefix, fixing tab indentation matching issues
    
    Parameters:
        generated_code: Complete code generated by the model
        prompt: Original prompt prefix
        
    Returns:
        String containing only the newly added code
    """
    # Normalize whitespace: convert tabs to 4 spaces to avoid matching issues
    prompt_normalized = prompt.replace('\t', '    ')
    generated_code_normalized = generated_code.replace('\t', '    ')
    
    # First attempt to fully match the entire prompt
    prompt_pos = generated_code_normalized.find(prompt_normalized)
    if prompt_pos != -1:
        prompt_pos += len(prompt_normalized)
    if prompt_pos == -1:
        # If full match fails, try matching suffixes of different lengths (from longest to shortest)
        # This helps handle potential truncation or minor discrepancies
        max_suffix_length = min(len(prompt_normalized), 200)  # Max 200 characters to try
        min_suffix_length = 20  # Min 20 characters to try
        
        for suffix_length in range(max_suffix_length, min_suffix_length, -10):
            suffix = prompt_normalized[-suffix_length:]
            prompt_pos = generated_code_normalized.find(suffix)
            if prompt_pos != -1:
                prompt_pos += len(suffix)  # Move to the end of the suffix
                break
    
    if prompt_pos != -1:
        # Match found: extract code after the match
        code_after_prompt = generated_code[prompt_pos:]
    else:
        # No match found: use the entire generated code
        print("Warning: Could not find the end of the prompt prefix in generated code. Using full generated content as completion.")
        if generated_code.find("def") == -1:
            return ""
        generated_code = generated_code[generated_code.find("def"):]
        generated_code = generated_code[generated_code.find("\n"):]
        code_after_prompt = generated_code
    
    # Find the earliest EOS marker and truncate
    min_index = None
    for marker in EOS_MARKERS:
        index = code_after_prompt.find(marker)
        if index != -1:
            if min_index is None or index < min_index:
                min_index = index
    
    # Truncate the result
    if min_index is not None:
        extracted_code = code_after_prompt[:min_index]
    else:
        extracted_code = code_after_prompt  # Use full content if no EOS marker found
    
    # Clean up extracted code: remove leading blank lines while preserving indentation integrity
    extracted_code = extracted_code.lstrip('\n').rstrip()
    # print(extracted_code)
    return extracted_code

# Load HumanEval dataset
problems = read_problems()

# Test each max_new_tokens value
for max_new_tokens in MAX_NEW_TOKENS_LIST:
    print(f"\nStarting test with max_new_tokens = {max_new_tokens}")
    
    # List to store generated results
    results = []
    
    # Process each problem
    with multi_timer() as timer:
        for problem_id, problem in problems.items():
            print(f"Processing problem: {problem_id}")
            
            # Extract problem prompt
            prompt = problem['prompt']
            
            # Build new prompt template, ensuring it starts with ```python
            prompt_template = f"""
You are an intelligent programming assistant to produce Python algorithmic solutions. 
Please complete the following Python function. Your response must start with ```python, 
followed by the provided code prefix, then your continuation, and end with ```.
{prompt}"""
            
            # Call API to generate code
            timer.start()
            generated_text = call_mercury_api(prompt_template, max_new_tokens)
            timer.stop()
            
            # Process generated text to extract code section
            # Find start and end of code block
            start_marker = "```python"
            end_marker = "```"
            
            start_idx = generated_text.find(start_marker)
            if start_idx != -1:
                # Skip the start marker
                start_idx += len(start_marker)
                end_idx = generated_text.find(end_marker, start_idx)
                
                if end_idx != -1:
                    generated_code = generated_text[start_idx:end_idx].strip()
                else:
                    generated_code = generated_text[start_idx:].strip()
            else:
                # If no code block markers found, use entire generated text
                generated_code = generated_text
            
            # Extract the part beyond the prompt prefix as completion
            extracted_code = extract_completion(generated_code, prompt)
            print(extracted_code)
            
            # Save result
            results.append({
                "task_id": problem_id,
                "completion": extracted_code
            })
        
        total_time = timer.get_total()
    
    # Define result file path
    output_file = f"humaneval_mercury_max_tokens_{max_new_tokens}.jsonl"
    append_number_to_file("speed.txt", f"{total_time} {output_file}")
    
    # Save generated results to JSONL file
    with open(output_file, mode='w', encoding='utf-8') as f:
        for result in results:
            json.dump(result, f)
            f.write('\n')
    
    # Evaluate generated code
    print("Starting evaluation...")
    evaluation_results = evaluate_functional_correctness(output_file, k=[1, 10, 100], n_workers=4)
    
    # Print evaluation results
    print(f"\nEvaluation Results for max_new_tokens={max_new_tokens}:")
    for k, v in evaluation_results.items():
        print(f"k={k}: {v}")
    
    # Save evaluation results
    with open("evaluation_summary.txt", "a") as f:
        f.write(f"max_new_tokens={max_new_tokens}\n")
        for k, v in evaluation_results.items():
            f.write(f"k={k}: {v}\n")
        f.write("\n")
    
    print(f"Results saved to {output_file}")

print("All tests completed!")