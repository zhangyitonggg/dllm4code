import re
import argparse
import json
import logging
import signal
import time
import torch
import os
import copy
import subprocess
from dataclasses import dataclass, field
from functools import lru_cache
from pathlib import Path
from typing import Any, NamedTuple
from datetime import datetime 
from contextlib import contextmanager

import datasets
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer

# å¯¼å…¥LLaDAç‰¹æœ‰çš„ç”Ÿæˆå‡½æ•°
from llada_generate import generate


# ================================ é…ç½®å’Œå¸¸é‡ ================================

@dataclass(frozen=True)
class GenerationConfig:
    """ä»£ç ç”Ÿæˆé…ç½®ç±»ï¼Œæ‰€æœ‰ç”Ÿæˆå‚æ•°åœ¨æ­¤æ§åˆ¶"""
    temperature: float = 0.1        # ç”Ÿæˆæ¸©åº¦
    steps: int = 512                # ç”Ÿæˆæ­¥éª¤æ•°
    gen_length: int = 512           # ç”Ÿæˆé•¿åº¦
    block_length: int = 16         # å—é•¿åº¦
    cfg_scale: float = 0.0          # åˆ†ç±»å™¨å¼•å¯¼å°ºåº¦
    remasking: str = 'low_confidence'  # é‡æ©ç ç­–ç•¥


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®ç±»ï¼Œæ‰€æœ‰è¯„ä¼°å‚æ•°åœ¨æ­¤æ§åˆ¶"""
    model_path: str = "/xxx/xxx/LLaDA-8B-Instruct"  # LLaDAæ¨¡å‹è·¯å¾„
    k: int = 1                     # pass@kä¸­çš„kå€¼
    output_dir: str = "LiveCodeBench_llada_results"  # è¾“å‡ºç›®å½•
    max_samples: int | None = None  # æœ€å¤§æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
    max_new_tokens: int = 512      # æœ€å¤§æ–°ç”Ÿæˆtokenæ•°
    timeout_seconds: int = 5       # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    generation_config: GenerationConfig = field(default_factory=GenerationConfig)  # ç”Ÿæˆé…ç½®
    device: str = "cuda:7"         # è¿è¡Œè®¾å¤‡
    custom_evaluator_path: str = "/xxx/xxx/LiveCodeBench/lcb_runner"  # è¯„ä¼°å™¨è·¯å¾„
    run_evaluator: bool = True     # æ˜¯å¦è‡ªåŠ¨è¿è¡Œè¯„ä¼°å™¨


class GenerationResult(NamedTuple):
    """å•ä¸ªç”Ÿæˆæ ·æœ¬çš„ç»“æœ"""
    question_id: str
    code_list: list[str]  # å­˜å‚¨kæ¬¡ç”Ÿæˆçš„ä»£ç 


class EvaluationSummary(NamedTuple):
    """è¯„ä¼°æ€»ç»“ç»“æœ"""
    model: str
    model_type: str
    generation_time: float
    average_time_per_sample: float
    total_samples: int


# ================================ è‡ªå®šä¹‰å¼‚å¸¸ ================================

class LiveCodeBenchEvaluationError(Exception):
    """LiveCodeBenchè¯„ä¼°è¿‡ç¨‹ä¸­çš„åŸºç¡€å¼‚å¸¸ç±»"""
    pass


class ModelLoadError(LiveCodeBenchEvaluationError):
    """æ¨¡å‹åŠ è½½å¤±è´¥å¼‚å¸¸"""
    pass


class DatasetLoadError(LiveCodeBenchEvaluationError):
    """æ•°æ®é›†åŠ è½½å¤±è´¥å¼‚å¸¸"""
    pass


class CodeGenerationError(LiveCodeBenchEvaluationError):
    """ä»£ç ç”Ÿæˆå¤±è´¥å¼‚å¸¸"""
    pass


# ================================ è¾…åŠ©å‡½æ•° ================================

@contextmanager
def multi_timer():
    """å¤šè®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºç»Ÿè®¡æ€»æ‰§è¡Œæ—¶é—´"""
    total_time = 0.0
    start_time = None
    
    class Timer:
        def start(self):
            nonlocal start_time
            if start_time is not None:
                raise RuntimeError("è®¡æ—¶å™¨å·²ç»å¼€å§‹ï¼Œè¯·å…ˆåœæ­¢å½“å‰è®¡æ—¶")
            start_time = time.perf_counter()
        
        def stop(self):
            nonlocal total_time, start_time
            if start_time is None:
                raise RuntimeError("è¯·å…ˆå¼€å§‹è®¡æ—¶")
            end_time = time.perf_counter()
            total_time += end_time - start_time
            start_time = None
            
        def get_total(self):
            """è·å–å½“å‰ç´¯è®¡çš„æ€»æ—¶é—´"""
            return total_time
    
    try:
        yield Timer()
    finally:
        print(f"æ‰€æœ‰è®¡æ—¶è¯­å¥çš„æ€»æ‰§è¡Œæ—¶é—´: {total_time:.6f} ç§’")


def append_number_to_file(filename, number):
    """å°†æ•°å­—è¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾"""
    try:
        with open(filename, 'a') as file:
            file.write(f"{number}\n")
        print(f"æˆåŠŸå°†æ•°å­— {number} è¿½åŠ åˆ°æ–‡ä»¶ {filename}")
    except Exception as e:
        print(f"è¿½åŠ æ•°å­—æ—¶å‡ºé”™: {e}")


# ================================ æ ¸å¿ƒåŠŸèƒ½ç±» ================================

class ModelInterface:
    """æ¨¡å‹æ¥å£ç±»ï¼Œå°è£…LLaDAæ¨¡å‹åŠ è½½å’Œä»£ç ç”ŸæˆåŠŸèƒ½"""

    def __init__(self, model_path: str, device: str) -> None:
        """åˆå§‹åŒ–æ¨¡å‹æ¥å£"""
        self.model_path = model_path
        self.device = device
        self._model: AutoModel | None = None
        self._tokenizer: AutoTokenizer | None = None
        # LLaDAç‰¹æœ‰çš„EOSæ ‡è®°
        self._eos_markers = [
            "<|endoftext|>",
            "<|eot_id|>",
            "\n```",
            "\nassert ",
            "# Example usage:",
        ]
        self._load_model()

    def _load_model(self) -> None:
        """åŠ è½½LLaDAæ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"æ­£åœ¨åŠ è½½LLaDAæ¨¡å‹: {self.model_path}")

        try:
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            self._model = AutoModel.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
            
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            self._model = self._model.to(self.device).eval()
            logger.info(f"âœ… LLaDAæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

        except Exception as e:
            error_msg = f"LLaDAæ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            logger.error(error_msg)
            raise ModelLoadError(error_msg) from e

    @property
    def model(self) -> AutoModel:
        """è·å–æ¨¡å‹å®ä¾‹"""
        if self._model is None:
            raise ModelLoadError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
        return self._model

    @property
    def tokenizer(self) -> AutoTokenizer:
        """è·å–åˆ†è¯å™¨å®ä¾‹"""
        if self._tokenizer is None:
            raise ModelLoadError("åˆ†è¯å™¨æœªæ­£ç¡®åŠ è½½")
        return self._tokenizer

    def generate_code(
        self,
        question:dict,
        config: GenerationConfig,
        max_new_tokens: int = 512,
    ) -> str:
        """ç”Ÿæˆä»£ç ï¼ˆLLaDAç‰¹æœ‰æ–¹æ³•ï¼‰"""
        # æ„å»ºæç¤ºæ¨¡æ¿ - é€‚é…LLaDAçš„æ ¼å¼
        def get_question_template_answer(question):
            if question["starter_code"]:
                inner = "You will use the following starter code to write the solution to the problem and enclose your code within delimiters.```python  ```. You might only need to fill in the given function and return the given list of number. Here is the starter code:" + "\n" + question["starter_code"]
            else:
                inner = "Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows. Ensure that when the python program runs, it reads the inputs, runs the algorithm and writes output to STDOUT.\n```python\n# YOUR CODE HERE\n```\n"
            
            prompt_template = f"""<|startoftext|><|start_header_id|>user<|end_header_id|>

You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.

Question: {question["question_content"]}
{inner}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```python
"""
            if question["starter_code"]:
                prompt_template += question["starter_code"]
            
            return prompt_template
        
        prompt_template = get_question_template_answer(question)
        
        # Tokenize
        inputs = self.tokenizer(
            prompt_template,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        )
        
        input_ids = inputs.input_ids.to(device=self.device)
        attention_mask = inputs.attention_mask.to(device=self.device)
        
        # ç”Ÿæˆä»£ç  - ä½¿ç”¨LLaDAç‰¹æœ‰çš„generateå‡½æ•°
        with torch.no_grad():
            output = generate(
                self.model,
                input_ids,
                steps=config.steps,
                gen_length=config.gen_length,
                block_length=config.block_length,
                temperature=config.temperature,
                cfg_scale=config.cfg_scale,
                remasking=config.remasking
            )
        
        # è§£ç ç”Ÿæˆçš„å†…å®¹ï¼ˆæ’é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
        generated_text = self.tokenizer.batch_decode(
            output[:, input_ids.shape[1]:],  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            skip_special_tokens=True
        )[0]
        print(generated_text)
        # æ‰¾åˆ°æœ€æ—©å‡ºç°çš„EOSæ ‡è®°å¹¶æˆªå–
        eos_markers = copy.deepcopy(self._eos_markers)
        if question["starter_code"]:
            eos_markers.extend(["\nif __name__","\ndef main(","\nprint("])
        min_index = None
        for marker in eos_markers:
            index = generated_text.find(marker)
            if index != -1:
                if min_index is None or index < min_index:
                    min_index = index
        
        # æˆªå–ç»“æœ
        if min_index is not None:
            extracted_code = generated_text[:min_index]
        else:
            extracted_code = generated_text  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°EOSæ ‡è®°ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹
            
        # ç»„åˆ starter code å’Œç”Ÿæˆçš„ä»£ç 
        if question["starter_code"]:
            full_code = question["starter_code"] + "\n" + extracted_code
        else:
            full_code = extracted_code
        print(full_code)
        return full_code


class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨ç±»"""

    @staticmethod
    @lru_cache(maxsize=1)
    def load_LiveCodeBench_dataset(max_samples: int | None = None) -> datasets.Dataset:
        """åŠ è½½LiveCodeBenchæ•°æ®é›†ï¼ˆå¸¦ç¼“å­˜ï¼‰ï¼Œå¹¶ç­›é€‰2024å¹´10æœˆä¹‹åçš„æ ·æœ¬"""
        try:
            # åŠ è½½åŸå§‹æ•°æ®é›†
            dataset = load_dataset("/xxx/xxx/code_generation_lite", version_tag="release_v6", trust_remote_code=True)["test"]
            logger.info(f"âœ… åŸå§‹LiveCodeBenchæ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")

            # å®šä¹‰æ—¥æœŸç­›é€‰å‡½æ•°ï¼šåªä¿ç•™2024å¹´10æœˆ1æ—¥åŠä¹‹åçš„æ ·æœ¬
            def filter_by_date(example):
                # è·å–æ ·æœ¬ä¸­çš„æ¯”èµ›æ—¥æœŸ
                contest_date_str = example.get("contest_date")
                if not contest_date_str:  # æ— æ—¥æœŸä¿¡æ¯çš„æ ·æœ¬æ’é™¤
                    return False
                
                try:
                    # è§£ææ—¥æœŸï¼ˆæ ¼å¼å¦‚ï¼š'2023-08-21T00:00:00'ï¼‰
                    contest_date = datetime.strptime(contest_date_str, '%Y-%m-%dT%H:%M:%S')
                    # è®¾å®šç­›é€‰é˜ˆå€¼ï¼š2024å¹´10æœˆ1æ—¥
                    cutoff_date = datetime(1970, 1, 1)
                    # ä¿ç•™æ—¥æœŸåœ¨é˜ˆå€¼ä¹‹åçš„æ ·æœ¬
                    return contest_date >= cutoff_date
                except ValueError:  # æ—¥æœŸæ ¼å¼é”™è¯¯çš„æ ·æœ¬æ’é™¤
                    logger.warning(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {contest_date_str}ï¼Œå·²æ’é™¤è¯¥æ ·æœ¬")
                    return False

            # åº”ç”¨ç­›é€‰
            filtered_dataset = dataset.filter(filter_by_date)
            logger.info(f"ç­›é€‰åï¼ˆ2024å¹´10æœˆä¹‹åï¼‰çš„æ ·æœ¬æ•°: {len(filtered_dataset)}")

            # å¤„ç†æœ€å¤§æ ·æœ¬æ•°é™åˆ¶
            if max_samples is not None:
                actual_max = min(max_samples, len(filtered_dataset))
                filtered_dataset = filtered_dataset.select(range(actual_max))
                logger.info(f"å·²é™åˆ¶æœ€å¤§æ ·æœ¬æ•°ä¸º: {actual_max}")

            return filtered_dataset

        except Exception as e:
            error_msg = f"LiveCodeBenchæ•°æ®é›†åŠ è½½å¤±è´¥: {e}"
            logger.error(error_msg)
            raise DatasetLoadError(error_msg) from e


class LiveCodeBenchGenerator:
    """LiveCodeBenchä»£ç ç”Ÿæˆå™¨ä¸»ç±»"""

    def __init__(self, config: EvaluationConfig) -> None:
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = config
        self.model_interface = ModelInterface(config.model_path, config.device)
        self.dataset_loader = DatasetLoader()

    def generate(self) -> tuple[EvaluationSummary, str]:
        """æ‰§è¡Œä»£ç ç”Ÿæˆå¹¶ä¿å­˜ç»“æœ"""
        try:
            # åŠ è½½æ•°æ®é›†
            dataset = self.dataset_loader.load_LiveCodeBench_dataset(self.config.max_samples)
            logger.info(f"æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")

            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(self.config.output_dir)
            output_path.mkdir(exist_ok=True)

            # å¼€å§‹ç”Ÿæˆä»£ç 
            logger.info(f"å¼€å§‹ä½¿ç”¨LLaDAæ¨¡å‹ç”Ÿæˆä»£ç ï¼ˆk={self.config.k}ï¼‰...")
            results: list[GenerationResult] = []
            start_time = time.time()
            
            # åˆå§‹åŒ–è®¡æ—¶å™¨
            with multi_timer() as timer:
                for i, sample in enumerate(tqdm(dataset, desc="ä»£ç ç”Ÿæˆè¿›åº¦")):
                    # è®¡æ—¶æ¯ä¸ªæ ·æœ¬çš„å¤„ç†
                    timer.start()
                    result = self._generate_single_sample(sample)
                    timer.stop()
                    
                    results.append(result)

                    # å®šæœŸæ‰“å°è¿›åº¦
                    if (i + 1) % 10 == 0:
                        logger.info(f"è¿›åº¦: {i+1}/{len(dataset)}")
            
            # ä¿å­˜æ€»æ—¶é—´
            total_time = timer.get_total()
            
            # ä¿å­˜ç”Ÿæˆç»“æœå¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯
            output_file = self._save_generation_results(results, start_time, output_path)
            
            # è®°å½•ç”Ÿæˆé€Ÿåº¦
            speed_info = f"{total_time} {output_file}"
            append_number_to_file("llada_speed.txt", speed_info)
            
            # ç”Ÿæˆæ€»ç»“ä¿¡æ¯
            summary = EvaluationSummary(
                model=self.config.model_path,
                model_type="llada",
                generation_time=total_time,
                average_time_per_sample=total_time / len(results) if results else 0,
                total_samples=len(results)
            )
            
            # æ‰“å°ç»“æœæ‘˜è¦
            self._print_generation_summary(summary, output_file)
            
            # å¦‚æœéœ€è¦ï¼Œè¿è¡Œè‡ªå®šä¹‰è¯„ä¼°å™¨
            if self.config.run_evaluator:
                self.run_custom_evaluator(output_file)
                
            return summary, output_file

        except Exception as e:
            if isinstance(e, LiveCodeBenchEvaluationError):
                raise
            error_msg = f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}"
            logger.error(error_msg)
            raise LiveCodeBenchEvaluationError(error_msg) from e

    def _generate_single_sample(self, sample: dict[str, Any]) -> GenerationResult:
        """ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆkæ¬¡ä»£ç """
        # è¿›è¡Œkæ¬¡ä»£ç ç”Ÿæˆ
        generated_codes = []
        for i in range(self.config.k):
            # ç”Ÿæˆä»£ç 
            try:
                generated_content = self.model_interface.generate_code(
                    sample, self.config.generation_config, self.config.max_new_tokens
                )

                # ä»£ç æ¸…ç†
                generated_code = generated_content
                logger.debug(f"ç”Ÿæˆçš„ä»£ç : {generated_code}")
            except Exception as e:
                logger.warning(f"ä»»åŠ¡{sample['question_id']}ç¬¬{i+1}æ¬¡ä»£ç ç”Ÿæˆå¤±è´¥: {e}")
                generated_code = "# ä»£ç ç”Ÿæˆå¤±è´¥\npass"

            generated_codes.append(generated_code)

        return GenerationResult(
            question_id=str(sample["question_id"]),
            code_list=generated_codes
        )

    def _save_generation_results(
        self,
        results: list[GenerationResult],
        start_time: float,
        output_path: Path,
    ) -> str:
        """ä¿å­˜ç”Ÿæˆç»“æœä¸ºç¬¦åˆè‡ªå®šä¹‰è¯„ä¼°å™¨è¦æ±‚çš„æ ¼å¼"""
        # è½¬æ¢ä¸ºè¦æ±‚çš„JSONæ ¼å¼
        output_data = [
            {
                "question_id": result.question_id,
                "code_list": result.code_list
            }
            for result in results
        ]
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«ç”Ÿæˆå‚æ•°ï¼‰
        params = self.config.generation_config
        output_filename = f"output_llada_steps{params.steps}_gen{params.gen_length}_block{params.block_length}_temp{params.temperature}_k{self.config.k}.json"
        output_file = output_path / output_filename
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
            
        return str(output_file.absolute())

    def run_custom_evaluator(self, output_file: str) -> None:
        """è¿è¡ŒLiveCodeBenchçš„è‡ªå®šä¹‰è¯„ä¼°å™¨"""
        logger.info(f"å¼€å§‹ä½¿ç”¨è‡ªå®šä¹‰è¯„ä¼°å™¨è¯„ä¼°ç”Ÿæˆçš„ä»£ç ...")
        
        # æ„å»ºè¯„ä¼°å‘½ä»¤
        evaluator_script = os.path.join(self.config.custom_evaluator_path, "runner", "custom_evaluator.py")
        command = [
            "python", "-m", "lcb_runner.runner.custom_evaluator",
            "--custom_output_file", output_file
        ]
        
        try:
            # è¿è¡Œè¯„ä¼°å™¨
            result = subprocess.run(
                command,
                check=True,
                capture_output=True,
                text=True,
                cwd="/xxx/xxx/LiveCodeBench"
            )
            
            # è¾“å‡ºè¯„ä¼°ç»“æœ
            logger.info("è¯„ä¼°å™¨è¾“å‡º:")
            logger.info(result.stdout)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            eval_result_file = f"{output_file}.eval_results.txt"
            with open(eval_result_file, "w", encoding="utf-8") as f:
                f.write(result.stdout)
                
            logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_result_file}")
            
        except subprocess.CalledProcessError as e:
            logger.error(f"è¯„ä¼°å™¨è¿è¡Œå¤±è´¥: {e.stderr}")
        except Exception as e:
            logger.error(f"è¿è¡Œè¯„ä¼°å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    @staticmethod
    def _print_generation_summary(
        summary: EvaluationSummary,
        output_file: str,
    ) -> None:
        """æ‰“å°ç”Ÿæˆç»“æœæ‘˜è¦"""
        logger.info("=" * 50)
        logger.info(f"LLaDAæ¨¡å‹ä»£ç ç”Ÿæˆå®Œæˆï¼")
        logger.info("=" * 50)
        logger.info(f"æ¨¡å‹: {summary.model}")
        logger.info(f"æ¨¡å‹ç±»å‹: LLaDA")
        logger.info(f"æ€»æ ·æœ¬æ•°: {summary.total_samples}")
        logger.info(f"ç”Ÿæˆç”¨æ—¶: {summary.generation_time:.2f}ç§’")
        logger.info(f"å¹³å‡æ¯æ ·æœ¬: {summary.average_time_per_sample:.2f}ç§’")
        logger.info(f"ç”Ÿæˆç»“æœæ–‡ä»¶: {output_file}")
        logger.info("=" * 50)


# ================================ å‘½ä»¤è¡Œæ¥å£ ================================

def create_arg_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="LiveCodeBench LLaDAæ¨¡å‹ä»£ç ç”Ÿæˆå™¨ï¼Œå‚æ•°ç”±é…ç½®ç±»æ§åˆ¶",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ«",
    )
    
    parser.add_argument(
        "--no-evaluator",
        action="store_true",
        help="ä¸è‡ªåŠ¨è¿è¡Œè‡ªå®šä¹‰è¯„ä¼°å™¨",
    )

    return parser


def setup_logging(log_level: str) -> None:
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=getattr(logging, log_level),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def main() -> None:
    """ä¸»å‡½æ•°"""
    parser = create_arg_parser()
    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)

    # åˆ›å»ºé…ç½®å®ä¾‹
    config = EvaluationConfig(
        run_evaluator=not args.no_evaluator  # ä»…è¿™ä¸ªå‚æ•°æ¥è‡ªå‘½ä»¤è¡Œ
    )

    try:
        # åˆ›å»ºç”Ÿæˆå™¨å¹¶è¿è¡Œ
        generator = LiveCodeBenchGenerator(config)
        summary, output_file = generator.generate()

        # æ‰“å°æˆåŠŸä¿¡æ¯
        print(f"\nğŸ‰ LLaDAæ¨¡å‹ä»£ç ç”ŸæˆæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“Š ç”Ÿæˆæ ·æœ¬æ•°: {summary.total_samples}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_file}")
        if config.run_evaluator:
            print(f"âœ… å·²è‡ªåŠ¨è¿è¡Œè¯„ä¼°å™¨")

    except LiveCodeBenchEvaluationError as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        print(f"\nâŒ ä»£ç ç”Ÿæˆå¤±è´¥: {e}")
        exit(1)
    except Exception as e:
        logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {e}")
        print(f"\nâŒ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        exit(1)


# ================================ æ—¥å¿—é…ç½® ================================

logger = logging.getLogger(__name__)


# ================================ ä¸»ç¨‹åºå…¥å£ ================================

if __name__ == "__main__":
    main()